from datetime import datetime

from flask import Flask, jsonify, request
from apscheduler.schedulers.background import BackgroundScheduler
from apscheduler.jobstores.memory import MemoryJobStore
from apscheduler.executors.pool import ProcessPoolExecutor
from apscheduler.triggers.cron import CronTrigger

import ace
import ace_cli
import ace_config as config
import ace_core
import ace_db
from ace_data_models import (
    RepsetDiffTask,
    SchemaDiffTask,
    SpockDiffTask,
    TableDiffTask,
    TableRepairTask,
)
from ace_exceptions import AceException
from ace_timeparse import parse_time_string

app = Flask(__name__)

# apscheduler setup
scheduler = BackgroundScheduler(
    jobstores={"default": MemoryJobStore()},
    executors={"default": ProcessPoolExecutor(32)},
)


"""
API endpoint for initiating a table diff operation.

This endpoint accepts the following GET parameters:
- cluster_name (required): Name of the cluster
- table_name (required): Name of the table to diff
- dbname (optional): Name of the database
- block_rows (optional): Number of rows per block (default: config.BLOCK_ROWS_DEFAULT)
- max_cpu_ratio (optional): Max CPU usage ratio (default: config.MAX_CPU_RATIO_DEFAULT)
- output (optional): Output format, default is 'json'
- nodes (optional): Nodes to include in diff, default is 'all'
- batch_size (optional): Batch size for processing (default: config.BATCH_SIZE_DEFAULT)
- quiet (optional): Whether to suppress output, default is False

Returns:
    JSON response with task_id and submitted_at timestamp on success,
    or an error message on failure.
"""


@app.route("/ace/table-diff", methods=["GET"])
def table_diff_api():
    cluster_name = request.args.get("cluster_name")
    table_name = request.args.get("table_name")
    dbname = request.args.get("dbname", None)
    block_rows = request.args.get("block_rows", config.BLOCK_ROWS_DEFAULT)
    max_cpu_ratio = request.args.get("max_cpu_ratio", config.MAX_CPU_RATIO_DEFAULT)
    output = request.args.get("output", "json")
    nodes = request.args.get("nodes", "all")
    batch_size = request.args.get("batch_size", config.BATCH_SIZE_DEFAULT, type=int)
    quiet = request.args.get("quiet", False)

    if not cluster_name or not table_name:
        return jsonify({"error": "cluster_name and table_name are required parameters"})

    task_id = ace_db.generate_task_id()

    try:
        raw_args = TableDiffTask(
            cluster_name=cluster_name,
            _table_name=table_name,
            _dbname=dbname,
            block_rows=block_rows,
            max_cpu_ratio=max_cpu_ratio,
            output=output,
            _nodes=nodes,
            batch_size=batch_size,
            quiet_mode=quiet,
            skip_db_update=False,
        )

        raw_args.scheduler.task_id = task_id
        raw_args.scheduler.task_type = "table-diff"
        raw_args.scheduler.task_status = "RUNNING"
        raw_args.scheduler.started_at = datetime.now()
        td_task = ace.table_diff_checks(raw_args)

        ace_db.create_ace_task(task=td_task)
        scheduler.add_job(
            ace_core.table_diff,
            args=(td_task,),
        )

        return jsonify({"task_id": task_id, "submitted_at": datetime.now().isoformat()})
    except Exception as e:
        return jsonify({"error": str(e)})


"""
API endpoint for initiating a table repair operation.

This endpoint accepts the following GET parameters:
- cluster_name (required): Name of the cluster
- diff_file (required): Path to the diff file generated by a previous table diff
- source_of_truth (required): Source of truth for the data
- table_name (required): Name of the table to repair
- dbname (optional): Name of the database
- dry_run (optional): If True, simulates the repair without changes (default: False)
- quiet (optional): Whether to suppress output (default: False)
- generate_report (optional): If True, generates a detailed report of the repair
  (default: False)
- upsert_only (optional): If True, only performs upsert operations, skipping
  deletions (default: False)

Returns:
    JSON response with task_id and submitted_at timestamp on success,
    or an error message on failure.
"""


@app.route("/ace/table-repair", methods=["GET"])
def table_repair_api():
    cluster_name = request.args.get("cluster_name")
    diff_file = request.args.get("diff_file")
    source_of_truth = request.args.get("source_of_truth")
    table_name = request.args.get("table_name")
    dbname = request.args.get("dbname")
    dry_run = request.args.get("dry_run", False)
    quiet = request.args.get("quiet", False)
    generate_report = request.args.get("generate_report", False)
    upsert_only = request.args.get("upsert_only", False)

    if not cluster_name or not diff_file or not source_of_truth or not table_name:
        return jsonify(
            {
                "error": "cluster_name, diff_file, source_of_truth, and table_name"
                "are required parameters"
            }
        )

    task_id = ace_db.generate_task_id()

    try:
        raw_args = TableRepairTask(
            cluster_name=cluster_name,
            diff_file_path=diff_file,
            source_of_truth=source_of_truth,
            _table_name=table_name,
            _dbname=dbname,
            dry_run=dry_run,
            quiet_mode=quiet,
            generate_report=generate_report,
            upsert_only=upsert_only,
        )
        raw_args.scheduler.task_id = task_id
        raw_args.scheduler.task_type = "table-repair"
        raw_args.scheduler.task_status = "RUNNING"
        raw_args.scheduler.started_at = datetime.now()
        tr_task = ace.table_repair_checks(raw_args)
        ace_db.create_ace_task(task=tr_task)

        scheduler.add_job(ace_core.table_repair, args=(tr_task,))
        return jsonify({"task_id": task_id, "submitted_at": datetime.now().isoformat()})
    except Exception as e:
        return jsonify({"error": str(e)})


"""
API endpoint for rerunning a table diff operation.

This endpoint allows users to rerun a table diff operation based on a previous
diff file. It supports two different behaviors: multiprocessing and hostdb.

GET parameters:
    cluster_name (str): Name of the cluster (required)
    diff_file (str): Path to the diff file from a previous table diff (required)
    table_name (str): Name of the table to rerun the diff on (required)
    dbname (str): Name of the database (optional)
    quiet (bool): Whether to suppress output (optional, default: False)
    behavior (str): The behavior to use for rerunning
                    (optional, default: "multiprocessing")
                    Supported values: "multiprocessing", "hostdb"

Returns:
    JSON response with task_id and submitted_at timestamp on success,
    or an error message on failure.
"""


@app.route("/ace/table-rerun", methods=["GET"])
def table_rerun_api():
    cluster_name = request.args.get("cluster_name")
    diff_file = request.args.get("diff_file")
    table_name = request.args.get("table_name")
    dbname = request.args.get("dbname", None)
    quiet = request.args.get("quiet", False)
    behavior = request.args.get("behavior", "multiprocessing")

    if not cluster_name or not diff_file or not table_name:
        return jsonify(
            {
                "error": "cluster_name, diff_file, and table_name"
                "are required parameters"
            }
        )

    task_id = ace_db.generate_task_id()

    try:
        raw_args = TableDiffTask(
            cluster_name=cluster_name,
            _table_name=table_name,
            _dbname=dbname,
            block_rows=config.BLOCK_ROWS_DEFAULT,
            max_cpu_ratio=config.MAX_CPU_RATIO_DEFAULT,
            output="json",
            _nodes="all",
            batch_size=config.BATCH_SIZE_DEFAULT,
            quiet_mode=quiet,
            diff_file_path=diff_file,
        )
        raw_args.scheduler.task_id = task_id
        raw_args.scheduler.task_type = "table-rerun"
        raw_args.scheduler.task_status = "RUNNING"
        raw_args.scheduler.started_at = datetime.now()
        td_task = ace.table_diff_checks(raw_args)
        ace_db.create_ace_task(task=td_task)
    except Exception as e:
        return jsonify({"error": str(e)})

    try:
        if behavior == "multiprocessing":
            scheduler.add_job(ace_core.table_rerun_async, args=(td_task,))
            return jsonify(
                {"task_id": task_id, "submitted_at": datetime.now().isoformat()}
            )
        elif behavior == "hostdb":
            scheduler.add_job(ace_core.table_rerun_temptable, args=(td_task,))
            return jsonify(
                {"task_id": task_id, "submitted_at": datetime.now().isoformat()}
            )
        else:
            return jsonify({"error": f"Invalid behavior: {behavior}"})
    except Exception as e:
        return jsonify({"error": str(e)})


"""
Performs a repset diff operation on a specified cluster and repset.

API Endpoint: /ace/repset-diff
Method: GET

Query Parameters:
    cluster_name (str): Name of the cluster (required)
    repset_name (str): Name of the repset to diff (required)
    dbname (str): Name of the database (optional)
    block_rows (int): Number of rows per block (default: config.BLOCK_ROWS_DEFAULT)
    max_cpu_ratio (float): Maximum CPU usage ratio
                           (default: config.MAX_CPU_RATIO_DEFAULT)
    output (str): Output format (default: "json")
    nodes (str): Nodes to include in the diff (default: "all")
    batch_size (int): Size of each batch (default: config.BATCH_SIZE_DEFAULT)
    quiet (bool): Whether to suppress output (default: False)
    skip_tables (str): Comma-separated list of tables to skip (optional)

Returns:
    JSON object containing:
        task_id (str): Unique identifier for the submitted task
        submitted_at (str): ISO formatted timestamp of task submission
"""


@app.route("/ace/repset-diff", methods=["GET"])
def repset_diff_api():
    cluster_name = request.args.get("cluster_name")
    repset_name = request.args.get("repset_name")
    dbname = request.args.get("dbname", None)
    block_rows = request.args.get("block_rows", config.BLOCK_ROWS_DEFAULT)
    max_cpu_ratio = request.args.get("max_cpu_ratio", config.MAX_CPU_RATIO_DEFAULT)
    output = request.args.get("output", "json")
    nodes = request.args.get("nodes", "all")
    batch_size = request.args.get("batch_size", config.BATCH_SIZE_DEFAULT, type=int)
    quiet = request.args.get("quiet", False)
    skip_tables = request.args.get("skip_tables", None)

    if not cluster_name or not repset_name:
        return jsonify(
            {"error": "cluster_name and repset_name are required parameters"}
        )

    task_id = ace_db.generate_task_id()

    try:
        raw_args = RepsetDiffTask(
            cluster_name=cluster_name,
            _dbname=dbname,
            repset_name=repset_name,
            block_rows=block_rows,
            max_cpu_ratio=max_cpu_ratio,
            output=output,
            _nodes=nodes,
            batch_size=batch_size,
            quiet_mode=quiet,
            skip_tables=skip_tables,
            invoke_method="API",
        )

        raw_args.scheduler.task_id = task_id
        raw_args.scheduler.task_type = "repset-diff"
        raw_args.scheduler.task_status = "RUNNING"
        raw_args.scheduler.started_at = datetime.now()
        rd_task = ace.repset_diff_checks(raw_args)
        ace_db.create_ace_task(task=rd_task)
        scheduler.add_job(ace_core.repset_diff, args=(rd_task,))
        return jsonify({"task_id": task_id, "submitted_at": datetime.now().isoformat()})
    except Exception as e:
        return jsonify({"error": str(e)})


"""
Perform a Spock diff operation on a specified cluster.

This API endpoint initiates a Spock diff task and returns a task ID for tracking.

Parameters:
    cluster_name (str): Required. Name of the cluster to perform the diff on.
    dbname (str): Optional. Name of the database. Defaults to None.
    nodes (str): Optional. Nodes to include in the diff. Defaults to "all".
    quiet (bool): Optional. Whether to suppress output. Defaults to False.

Returns:
    JSON object containing:
        - task_id (str): Unique identifier for the submitted task.
        - submitted_at (str): ISO formatted timestamp of task submission.
"""


@app.route("/ace/spock-diff", methods=["GET"])
def spock_diff_api():
    cluster_name = request.args.get("cluster_name")
    dbname = request.args.get("dbname", None)
    nodes = request.args.get("nodes", "all")
    quiet = request.args.get("quiet", False)

    if not cluster_name:
        return jsonify({"error": "cluster_name is a required parameter"})

    task_id = ace_db.generate_task_id()

    try:
        raw_args = SpockDiffTask(
            cluster_name=cluster_name,
            _dbname=dbname,
            _nodes=nodes,
            quiet_mode=quiet,
        )

        raw_args.scheduler.task_id = task_id
        raw_args.scheduler.task_type = "spock-diff"
        raw_args.scheduler.task_status = "RUNNING"
        raw_args.scheduler.started_at = datetime.now()
        sd_task = ace.spock_diff_checks(raw_args)
        ace_db.create_ace_task(task=sd_task)
        scheduler.add_job(ace_core.spock_diff, args=(sd_task,))
        return jsonify({"task_id": task_id, "submitted_at": datetime.now().isoformat()})
    except Exception as e:
        return jsonify({"error": str(e)})


"""
Perform a schema diff operation on a specified cluster and schema.

This API endpoint initiates a schema diff task and returns a task ID for tracking.

Parameters:
    cluster_name (str): Required. Name of the cluster to perform the diff on.
    schema_name (str): Required. Name of the schema to diff.
    dbname (str): Optional. Name of the database. Defaults to None.
    nodes (str): Optional. Nodes to include in the diff. Defaults to "all".
    quiet (bool): Optional. Whether to suppress output. Defaults to False.

Returns:
    JSON object containing:
        - task_id (str): Unique identifier for the submitted task.
        - submitted_at (str): ISO formatted timestamp of task submission.
"""


@app.route("/ace/schema-diff", methods=["GET"])
def schema_diff_api():
    cluster_name = request.args.get("cluster_name")
    schema_name = request.args.get("schema_name")
    dbname = request.args.get("dbname", None)
    nodes = request.args.get("nodes", "all")
    quiet = request.args.get("quiet", False)

    task_id = ace_db.generate_task_id()

    if not cluster_name or not schema_name:
        return (
            jsonify({"error": "cluster_name and schema_name are required parameters"}),
            400,
        )

    try:
        raw_args = SchemaDiffTask(
            cluster_name=cluster_name,
            schema_name=schema_name,
            _dbname=dbname,
            _nodes=nodes,
            quiet_mode=quiet,
        )

        raw_args.scheduler.task_id = task_id
        raw_args.scheduler.task_type = "schema-diff"
        raw_args.scheduler.task_status = "RUNNING"
        raw_args.scheduler.started_at = datetime.now()
        sd_task = ace.schema_diff_checks(raw_args)
        ace_db.create_ace_task(task=sd_task)
        scheduler.add_job(ace_core.schema_diff, args=(sd_task,))
        return jsonify({"task_id": task_id, "submitted_at": datetime.now().isoformat()})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


"""
This API endpoint retrieves the status of a task given its task ID.

Parameters:
    task_id (str): Required. Unique identifier of the task to retrieve status for.

Returns:
    JSON object containing:
        - Task details if found, including status and other relevant information.
        - Error message if task_id is missing or task is not found.
"""


@app.route("/ace/task-status", methods=["GET"])
def task_status_api():
    task_id = request.args.get("task_id")

    if not task_id:
        return jsonify({"error": "task_id is a required parameter"})

    try:
        task_details = ace_db.get_ace_task_by_id(task_id)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

    if not task_details:
        return jsonify({"error": f"Task {task_id} not found"}), 404

    return jsonify(task_details)


def validate_table_diff_schedule(schedule_config):
    for schedule in schedule_config:
        if "table_name" not in schedule or "cluster_name" not in schedule:
            raise AceException(
                "table_name and cluster_name are required for every entry in "
                "schedule_config"
            )
        if "crontab_schedule" not in schedule:
            if "run_frequency" not in schedule:
                raise AceException(
                    "Either crontab_schedule or run_frequency must be specified "
                    "for every table in schedule_config"
                )
            else:
                try:
                    parse_time_string(schedule["run_frequency"])
                except Exception as e:
                    raise AceException(
                        f"Invalid run_frequency: {schedule['run_frequency']}. "
                        f"Error: {e}"
                    )
                schedule["crontab_schedule"] = None


def create_schedules():
    schedules = config.schedule_config

    for schedule in schedules:
        args = {
            "dbname": schedule.get("args", {}).get("dbname"),
            "block_rows": schedule.get("args", {}).get("block_rows"),
            "max_cpu_ratio": schedule.get("args", {}).get("max_cpu_ratio"),
            "batch_size": schedule.get("args", {}).get("batch_size"),
            "nodes": schedule.get("args", {}).get("nodes"),
        }
        cron_schedule = schedule.get("crontab_schedule", None)
        if cron_schedule:
            scheduler.add_job(
                ace_cli.table_diff_cli,
                CronTrigger.from_crontab(cron_schedule),
                args=[schedule["cluster_name"], schedule["table_name"]],
                kwargs=args,
            )
        else:
            interval = parse_time_string(schedule.get("run_frequency"))
            scheduler.add_job(
                ace_cli.table_diff_cli,
                'interval',
                weeks=interval.weeks if hasattr(interval, 'weeks') else 0,
                days=interval.days if hasattr(interval, 'days') else 0,
                hours=interval.hours if hasattr(interval, 'hours') else 0,
                minutes=interval.minutes if hasattr(interval, 'minutes') else 0,
                seconds=interval.seconds if hasattr(interval, 'seconds') else 0,
                args=[schedule["cluster_name"], schedule["table_name"]],
                kwargs=args,
            )


"""
Starts the ACE API server.

This function performs the following tasks:
1. Creates necessary database tables for ACE.
2. Starts the background scheduler for job management.
3. Runs the Flask application to serve the API.

The API server is configured to:
- Listen on localhost (127.0.0.1)
- Use port 5000
- Run in debug mode

Note: The scheduler is a BackgroundScheduler, so start() does not block execution.
Future versions may require manual event listening for job management.

Returns:
    None
"""


def start_ace():
    ace_db.create_ace_tables()
    validate_table_diff_schedule(config.schedule_config)
    create_schedules()

    # Since the scheduler is a BackgroundScheduler,
    # start() will not block
    scheduler.start()
    # ace.scheduler.add_listener(ace.error_listener, EVENT_JOB_ERROR)

    # A listener is needed for the upcoming 4.0.0 release
    # of apscheduler. We will need to manually listen to
    # the JOB_ADDED event and then run it. For now, using
    # a BackgroundScheduler with add_job() will automatically
    # run the job in the background.
    # scheduler.add_listener(listener, EVENT_JOB_ADDED)
    app.run(host="127.0.0.1", port=5000, debug=True)
